"""
–ü–∞—Ä—Å–µ—Ä –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Å Subito.it
"""
import aiohttp
from bs4 import BeautifulSoup
from typing import List, Dict, Any, Optional
import logging
import re
from urllib.parse import urljoin
from core.models import Listing

logger = logging.getLogger(__name__)


class SubitoParser:
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Å–∞–π—Ç–∞ Subito.it"""
    
    BASE_URL = "https://www.subito.it"
    SEARCH_URL = f"{BASE_URL}/annunci-italia/vendita/usato/"
    
    def __init__(self):
        self.session: Optional[aiohttp.ClientSession] = None
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'it-IT,it;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
    
    async def __aenter__(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Å—Å–∏–∏ –ø—Ä–∏ –≤—Ö–æ–¥–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç"""
        self.session = aiohttp.ClientSession(headers=self.headers)
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–µ—Å—Å–∏–∏ –ø—Ä–∏ –≤—ã—Ö–æ–¥–µ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        if self.session:
            await self.session.close()
    
    def _build_search_url(self, settings: Dict[str, Any]) -> str:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ URL –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å —É—á–µ—Ç–æ–º —Ñ–∏–ª—å—Ç—Ä–æ–≤"""
        params = []
        
        # –¶–µ–Ω–∞
        if settings.get("min_price"):
            params.append(f"ps={settings['min_price']}")
        if settings.get("max_price"):
            params.append(f"pe={settings['max_price']}")
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keywords = settings.get("keywords", [])
        if keywords:
            query = "+".join(keywords)
            url = f"{self.SEARCH_URL}?q={query}"
        else:
            url = self.SEARCH_URL
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if params:
            separator = "&" if "?" in url else "?"
            url += separator + "&".join(params)
        
        return url
    
    async def search_listings(
        self,
        settings: Dict[str, Any],
        max_results: int = 10
    ) -> List[Listing]:
        """
        –ü–æ–∏—Å–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –ø–æ –∑–∞–¥–∞–Ω–Ω—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
        
        Args:
            settings: –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            max_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        """
        if not self.session:
            raise RuntimeError("–°–µ—Å—Å–∏—è –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ async with.")
        
        search_url = self._build_search_url(settings)
        logger.info(f"üîç –ü–æ–∏—Å–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {search_url}")
        
        try:
            async with self.session.get(search_url) as response:
                if response.status != 200:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {response.status}")
                    return []
                
                html = await response.text()
                soup = BeautifulSoup(html, 'lxml')
                
                # –ü–∞—Ä—Å–∏–Ω–≥ —Å–ø–∏—Å–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
                listings = await self._parse_listings_page(soup, settings, max_results)
                
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(listings)}")
                return listings
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {e}")
            return []
    
    async def _parse_listings_page(
        self,
        soup: BeautifulSoup,
        settings: Dict[str, Any],
        max_results: int
    ) -> List[Listing]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–æ —Å–ø–∏—Å–∫–æ–º –æ–±—ä—è–≤–ª–µ–Ω–∏–π"""
        listings = []
        
        # –ü–æ–∏—Å–∫ –∫–∞—Ä—Ç–æ—á–µ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        # Subito.it –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∫–ª–∞—Å—Å—ã, –∏—â–µ–º –ø–æ data-id –∏–ª–∏ –¥—Ä—É–≥–∏–º –∞—Ç—Ä–∏–±—É—Ç–∞–º
        items = soup.find_all('div', class_=re.compile(r'item.*card|listing.*item', re.I))
        
        if not items:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫
            items = soup.find_all('a', href=re.compile(r'/.*\.htm$'))
        
        logger.info(f"üì¶ –ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(items)}")
        
        for item in items[:max_results]:
            try:
                listing = await self._parse_listing_card(item)
                if listing and self._matches_filters(listing, settings):
                    # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                    detailed_listing = await self._fetch_listing_details(listing.url)
                    if detailed_listing:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –ø—Ä–æ–¥–∞–≤—Ü–∞
                        if self._matches_seller_filters(detailed_listing, settings):
                            listings.append(detailed_listing)
                            if len(listings) >= max_results:
                                break
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ä—Ç–æ—á–∫–∏: {e}")
                continue
        
        return listings
    
    async def _parse_listing_card(self, item) -> Optional[Listing]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ä—Ç–æ—á–∫–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–∑ —Å–ø–∏—Å–∫–∞"""
        try:
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ URL
            link = item.find('a', href=re.compile(r'/.*\.htm$'))
            if not link:
                link = item if item.name == 'a' else None
            
            if not link or not link.get('href'):
                return None
            
            url = urljoin(self.BASE_URL, link['href'])
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ID –∏–∑ URL
            listing_id = url.split('/')[-1].replace('.htm', '').split('-')[-1]
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è
            title_elem = item.find(['h2', 'h3', 'p'], class_=re.compile(r'title|name', re.I))
            title = title_elem.get_text(strip=True) if title_elem else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω—ã
            price_elem = item.find(['span', 'p', 'div'], class_=re.compile(r'price|euro', re.I))
            price = 0.0
            if price_elem:
                price_text = price_elem.get_text(strip=True)
                price = self._extract_price(price_text)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            img_elem = item.find('img')
            image_url = None
            if img_elem:
                image_url = img_elem.get('src') or img_elem.get('data-src')
                if image_url and not image_url.startswith('http'):
                    image_url = urljoin(self.BASE_URL, image_url)
            
            return Listing(
                listing_id=listing_id,
                title=title,
                price=price,
                url=url,
                image_url=image_url
            )
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ä—Ç–æ—á–∫–∏: {e}")
            return None
    
    async def _fetch_listing_details(self, url: str) -> Optional[Listing]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –æ–±—ä—è–≤–ª–µ–Ω–∏–∏"""
        try:
            async with self.session.get(url) as response:
                if response.status != 200:
                    return None
                
                html = await response.text()
                soup = BeautifulSoup(html, 'lxml')
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ID
                listing_id = url.split('/')[-1].replace('.htm', '').split('-')[-1]
                
                # –ù–∞–∑–≤–∞–Ω–∏–µ
                title_elem = soup.find(['h1', 'h2'], class_=re.compile(r'title|heading', re.I))
                title = title_elem.get_text(strip=True) if title_elem else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
                
                # –¶–µ–Ω–∞
                price_elem = soup.find(['span', 'div'], class_=re.compile(r'price', re.I))
                price = 0.0
                if price_elem:
                    price = self._extract_price(price_elem.get_text(strip=True))
                
                # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                img_elem = soup.find('img', class_=re.compile(r'main.*image|gallery.*image', re.I))
                if not img_elem:
                    img_elem = soup.find('img', alt=True)
                image_url = None
                if img_elem:
                    image_url = img_elem.get('src') or img_elem.get('data-src')
                    if image_url and not image_url.startswith('http'):
                        image_url = urljoin(self.BASE_URL, image_url)
                
                # –û–ø–∏—Å–∞–Ω–∏–µ
                desc_elem = soup.find(['div', 'p'], class_=re.compile(r'description|content', re.I))
                description = desc_elem.get_text(strip=True) if desc_elem else ""
                
                # –õ–æ–∫–∞—Ü–∏—è
                location_elem = soup.find(['span', 'div'], class_=re.compile(r'location|city|town', re.I))
                location = location_elem.get_text(strip=True) if location_elem else ""
                
                # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–¥–∞–≤—Ü–µ
                seller_name = ""
                seller_url = ""
                seller_online_status = ""
                seller_listings_count = 0
                seller_registration_date = ""
                
                seller_elem = soup.find(['div', 'section'], class_=re.compile(r'seller|user|author', re.I))
                if seller_elem:
                    # –ò–º—è –ø—Ä–æ–¥–∞–≤—Ü–∞
                    name_elem = seller_elem.find(['span', 'a', 'p'], class_=re.compile(r'name|username', re.I))
                    if name_elem:
                        seller_name = name_elem.get_text(strip=True)
                        if name_elem.name == 'a' and name_elem.get('href'):
                            seller_url = urljoin(self.BASE_URL, name_elem['href'])
                    
                    # –°—Ç–∞—Ç—É—Å –æ–Ω–ª–∞–π–Ω
                    status_elem = seller_elem.find(['span', 'div'], class_=re.compile(r'online|status|active', re.I))
                    if status_elem:
                        seller_online_status = status_elem.get_text(strip=True)
                    
                    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
                    listings_elem = seller_elem.find(text=re.compile(r'annunci|pubblicati', re.I))
                    if listings_elem:
                        numbers = re.findall(r'\d+', listings_elem)
                        if numbers:
                            seller_listings_count = int(numbers[0])
                    
                    # –î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
                    reg_elem = seller_elem.find(text=re.compile(r'dal|da|registrato|pubblica da', re.I))
                    if reg_elem:
                        seller_registration_date = reg_elem.strip()
                
                # –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                date_elem = soup.find(['span', 'time'], class_=re.compile(r'date|published|time', re.I))
                published_date = date_elem.get_text(strip=True) if date_elem else ""
                
                # –ö–∞—Ç–µ–≥–æ—Ä–∏—è
                category_elem = soup.find(['span', 'a'], class_=re.compile(r'category|breadcrumb', re.I))
                category = category_elem.get_text(strip=True) if category_elem else ""
                
                return Listing(
                    listing_id=listing_id,
                    title=title,
                    price=price,
                    url=url,
                    image_url=image_url,
                    seller_name=seller_name,
                    seller_url=seller_url,
                    seller_online_status=seller_online_status,
                    location=location,
                    description=description,
                    published_date=published_date,
                    seller_listings_count=seller_listings_count,
                    seller_registration_date=seller_registration_date,
                    category=category
                )
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {e}")
            return None
    
    def _extract_price(self, price_text: str) -> float:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è —Ü–µ–Ω—ã"""
        try:
            # –£–¥–∞–ª—è–µ–º –≤—Å–µ –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä –∏ —Ç–æ—á–∫–∏/–∑–∞–ø—è—Ç–æ–π
            price_text = re.sub(r'[^\d,.]', '', price_text)
            # –ó–∞–º–µ–Ω—è–µ–º –∑–∞–ø—è—Ç—É—é –Ω–∞ —Ç–æ—á–∫—É
            price_text = price_text.replace(',', '.')
            return float(price_text) if price_text else 0.0
        except:
            return 0.0
    
    def _matches_filters(self, listing: Listing, settings: Dict[str, Any]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –æ–±—ä—è–≤–ª–µ–Ω–∏—è –±–∞–∑–æ–≤—ã–º —Ñ–∏–ª—å—Ç—Ä–∞–º"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–Ω—ã
        min_price = settings.get("min_price", 0)
        max_price = settings.get("max_price", float('inf'))
        
        if not (min_price <= listing.price <= max_price):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keywords = settings.get("keywords", [])
        if keywords:
            title_lower = listing.title.lower()
            if not any(keyword.lower() in title_lower for keyword in keywords):
                return False
        
        return True
    
    def _matches_seller_filters(self, listing: Listing, settings: Dict[str, Any]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ñ–∏–ª—å—Ç—Ä–∞–º –ø—Ä–æ–¥–∞–≤—Ü–∞"""
        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —É –ø—Ä–æ–¥–∞–≤—Ü–∞
        max_seller_listings = settings.get("max_seller_listings", float('inf'))
        if listing.seller_listings_count > max_seller_listings:
            return False
        
        # –ì–æ–¥ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–æ–¥–∞–≤—Ü–∞
        seller_year = settings.get("seller_registration_year")
        if seller_year and listing.seller_registration_date:
            year_match = re.search(r'\d{4}', listing.seller_registration_date)
            if year_match:
                reg_year = int(year_match.group())
                if reg_year < seller_year:
                    return False
        
        return True

