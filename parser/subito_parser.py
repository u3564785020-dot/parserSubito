"""
–ü–∞—Ä—Å–µ—Ä –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Å Subito.it
"""
import aiohttp
from bs4 import BeautifulSoup
from typing import List, Dict, Any, Optional
import logging
import re
from urllib.parse import urljoin
from core.models import Listing

logger = logging.getLogger(__name__)


class SubitoParser:
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Å–∞–π—Ç–∞ Subito.it"""
    
    BASE_URL = "https://www.subito.it"
    SEARCH_URL = f"{BASE_URL}/annunci-italia/vendita/usato/"
    
    def __init__(self):
        self.session: Optional[aiohttp.ClientSession] = None
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'it-IT,it;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
    
    async def __aenter__(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Å—Å–∏–∏ –ø—Ä–∏ –≤—Ö–æ–¥–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç"""
        self.session = aiohttp.ClientSession(headers=self.headers)
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–µ—Å—Å–∏–∏ –ø—Ä–∏ –≤—ã—Ö–æ–¥–µ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        if self.session:
            await self.session.close()
    
    def _build_search_url(self, settings: Dict[str, Any]) -> str:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ URL –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å —É—á–µ—Ç–æ–º —Ñ–∏–ª—å—Ç—Ä–æ–≤"""
        params = []
        
        # –¶–µ–Ω–∞
        if settings.get("min_price"):
            params.append(f"ps={settings['min_price']}")
        if settings.get("max_price"):
            params.append(f"pe={settings['max_price']}")
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keywords = settings.get("keywords", [])
        if keywords:
            query = "+".join(keywords)
            url = f"{self.SEARCH_URL}?q={query}"
        else:
            url = self.SEARCH_URL
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if params:
            separator = "&" if "?" in url else "?"
            url += separator + "&".join(params)
        
        return url
    
    async def search_listings(
        self,
        settings: Dict[str, Any],
        max_results: int = 10
    ) -> List[Listing]:
        """
        –ü–æ–∏—Å–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –ø–æ –∑–∞–¥–∞–Ω–Ω—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
        
        Args:
            settings: –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            max_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        """
        if not self.session:
            raise RuntimeError("–°–µ—Å—Å–∏—è –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ async with.")
        
        search_url = self._build_search_url(settings)
        logger.info(f"üîç –ü–æ–∏—Å–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {search_url}")
        logger.info(f"‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞: {settings}")
        
        try:
            logger.info(f"üåê –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ {search_url}")
            async with self.session.get(search_url) as response:
                logger.info(f"üì° –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç: {response.status}")
                
                if response.status != 200:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {response.status}")
                    return []
                
                html = await response.text()
                logger.info(f"üìÑ –†–∞–∑–º–µ—Ä HTML: {len(html)} —Å–∏–º–≤–æ–ª–æ–≤")
                
                soup = BeautifulSoup(html, 'lxml')
                
                # –ü–∞—Ä—Å–∏–Ω–≥ —Å–ø–∏—Å–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
                listings = await self._parse_listings_page(soup, settings, max_results)
                
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(listings)}")
                return listings
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {e}")
            import traceback
            logger.error(f"üìã Traceback: {traceback.format_exc()}")
            return []
    
    async def _parse_listings_page(
        self,
        soup: BeautifulSoup,
        settings: Dict[str, Any],
        max_results: int
    ) -> List[Listing]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–æ —Å–ø–∏—Å–∫–æ–º –æ–±—ä—è–≤–ª–µ–Ω–∏–π"""
        listings = []
        
        logger.info("üîç –ò—â–µ–º –æ–±—ä—è–≤–ª–µ–Ω–∏—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ...")
        
        # –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è Subito.it (2024-2025)
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤
        selectors_to_try = [
            # –û—Å–Ω–æ–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–∞—Ä—Ç–æ—á–µ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            'div[data-testid*="listing"]',
            'div[class*="listing"]',
            'div[class*="item"]',
            'article[class*="listing"]',
            'div[class*="card"]',
            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Å—Å—ã–ª–æ–∫ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
            'a[href*="/annunci/"]',
            'a[href*=".htm"]',
            # –û–±—â–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            'div[data-testid]',
            'article',
            'div[class*="AdItem"]'
        ]
        
        items = []
        for selector in selectors_to_try:
            found_items = soup.select(selector)
            logger.info(f"üîç –°–µ–ª–µ–∫—Ç–æ—Ä '{selector}': –Ω–∞–π–¥–µ–Ω–æ {len(found_items)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            if found_items:
                items = found_items
                logger.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ–ª–µ–∫—Ç–æ—Ä: {selector}")
                break
        
        if not items:
            logger.warning("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å –æ–±—ä—è–≤–ª–µ–Ω–∏—è–º–∏")
            # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ª—é–±—ã–µ —Å—Å—ã–ª–∫–∏
            items = soup.find_all('a', href=True)
            logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(items)} —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
        
        logger.info(f"üì¶ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(items)}")
        
        processed_count = 0
        for item in items[:max_results * 3]:  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–æ–ª—å—à–µ, —á–µ–º –Ω—É–∂–Ω–æ, –Ω–∞ —Å–ª—É—á–∞–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            try:
                processed_count += 1
                logger.debug(f"üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç {processed_count}/{min(len(items), max_results * 3)}")
                
                listing = await self._parse_listing_card(item)
                if listing and self._matches_filters(listing, settings):
                    logger.info(f"‚úÖ –û–±—ä—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ—à–ª–æ —Ñ–∏–ª—å—Ç—Ä—ã: {listing.title}")
                    # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                    detailed_listing = await self._fetch_listing_details(listing.url)
                    if detailed_listing:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –ø—Ä–æ–¥–∞–≤—Ü–∞
                        if self._matches_seller_filters(detailed_listing, settings):
                            listings.append(detailed_listing)
                            logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ: {detailed_listing.title}")
                            if len(listings) >= max_results:
                                break
                        else:
                            logger.debug(f"‚è≠Ô∏è –û–±—ä—è–≤–ª–µ–Ω–∏–µ –Ω–µ –ø—Ä–æ—à–ª–æ —Ñ–∏–ª—å—Ç—Ä—ã –ø—Ä–æ–¥–∞–≤—Ü–∞: {detailed_listing.title}")
                    else:
                        logger.debug(f"‚è≠Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {listing.title}")
                else:
                    logger.debug(f"‚è≠Ô∏è –û–±—ä—è–≤–ª–µ–Ω–∏–µ –Ω–µ –ø—Ä–æ—à–ª–æ –±–∞–∑–æ–≤—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–ª–µ–º–µ–Ω—Ç–∞ {processed_count}: {e}")
                continue
        
        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {processed_count}, –Ω–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(listings)}")
        return listings
    
    async def _parse_listing_card(self, item) -> Optional[Listing]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ä—Ç–æ—á–∫–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–∑ —Å–ø–∏—Å–∫–∞"""
        try:
            logger.debug(f"üîç –ü–∞—Ä—Å–∏–º —ç–ª–µ–º–µ–Ω—Ç: {item.name if hasattr(item, 'name') else type(item)}")
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ URL - –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã
            link = None
            
            # –°–ø–æ—Å–æ–± 1: –∏—â–µ–º —Å—Å—ã–ª–∫—É –≤–Ω—É—Ç—Ä–∏ —ç–ª–µ–º–µ–Ω—Ç–∞
            if item.name == 'a':
                link = item
            else:
                # –ò—â–µ–º —Å—Å—ã–ª–∫—É –≤–Ω—É—Ç—Ä–∏ —ç–ª–µ–º–µ–Ω—Ç–∞
                link = item.find('a', href=True)
            
            # –°–ø–æ—Å–æ–± 2: –∏—â–µ–º –ø–æ href –∞—Ç—Ä–∏–±—É—Ç—É
            if not link and hasattr(item, 'get') and item.get('href'):
                link = item
            
            if not link or not link.get('href'):
                logger.debug("‚è≠Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ –≤ —ç–ª–µ–º–µ–Ω—Ç–µ")
                return None
            
            url = urljoin(self.BASE_URL, link['href'])
            logger.debug(f"üîó –ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞: {url}")
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ID –∏–∑ URL
            listing_id = url.split('/')[-1].replace('.htm', '').split('-')[-1]
            if not listing_id or listing_id == 'htm':
                listing_id = url.split('/')[-2] if len(url.split('/')) > 1 else str(hash(url))
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è - –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            title = "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
            title_selectors = [
                'h1', 'h2', 'h3', 'h4',
                '[class*="title"]', '[class*="name"]', '[class*="heading"]',
                '[data-testid*="title"]', '[data-testid*="name"]'
            ]
            
            for selector in title_selectors:
                title_elem = item.find(selector) if hasattr(item, 'find') else None
                if title_elem and title_elem.get_text(strip=True):
                    title = title_elem.get_text(strip=True)
                    logger.debug(f"üìù –ù–∞–π–¥–µ–Ω–æ –Ω–∞–∑–≤–∞–Ω–∏–µ: {title}")
                    break
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ —ç–ª–µ–º–µ–Ω—Ç–µ, –∏—â–µ–º –≤ —Å—Å—ã–ª–∫–µ
            if title == "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è" and link:
                title_text = link.get_text(strip=True)
                if title_text:
                    title = title_text
                    logger.debug(f"üìù –ù–∞–∑–≤–∞–Ω–∏–µ –∏–∑ —Å—Å—ã–ª–∫–∏: {title}")
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω—ã - –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            price = 0.0
            price_selectors = [
                '[class*="price"]', '[class*="euro"]', '[class*="cost"]',
                '[data-testid*="price"]', 'span[class*="price"]', 'div[class*="price"]'
            ]
            
            for selector in price_selectors:
                price_elem = item.find(selector) if hasattr(item, 'find') else None
                if price_elem:
                    price_text = price_elem.get_text(strip=True)
                    extracted_price = self._extract_price(price_text)
                    if extracted_price > 0:
                        price = extracted_price
                        logger.debug(f"üí∞ –ù–∞–π–¥–µ–Ω–∞ —Ü–µ–Ω–∞: {price}")
                        break
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image_url = None
            img_elem = item.find('img') if hasattr(item, 'find') else None
            if img_elem:
                image_url = img_elem.get('src') or img_elem.get('data-src') or img_elem.get('data-lazy')
                if image_url and not image_url.startswith('http'):
                    image_url = urljoin(self.BASE_URL, image_url)
                logger.debug(f"üñºÔ∏è –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_url}")
            
            listing = Listing(
                listing_id=listing_id,
                title=title,
                price=price,
                url=url,
                image_url=image_url
            )
            
            logger.debug(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ: {listing.title} - {listing.price}‚Ç¨")
            return listing
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ä—Ç–æ—á–∫–∏: {e}")
            return None
    
    async def _fetch_listing_details(self, url: str) -> Optional[Listing]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –æ–±—ä—è–≤–ª–µ–Ω–∏–∏"""
        try:
            async with self.session.get(url) as response:
                if response.status != 200:
                    return None
                
                html = await response.text()
                soup = BeautifulSoup(html, 'lxml')
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ID
                listing_id = url.split('/')[-1].replace('.htm', '').split('-')[-1]
                
                # –ù–∞–∑–≤–∞–Ω–∏–µ
                title_elem = soup.find(['h1', 'h2'], class_=re.compile(r'title|heading', re.I))
                title = title_elem.get_text(strip=True) if title_elem else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
                
                # –¶–µ–Ω–∞
                price_elem = soup.find(['span', 'div'], class_=re.compile(r'price', re.I))
                price = 0.0
                if price_elem:
                    price = self._extract_price(price_elem.get_text(strip=True))
                
                # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                img_elem = soup.find('img', class_=re.compile(r'main.*image|gallery.*image', re.I))
                if not img_elem:
                    img_elem = soup.find('img', alt=True)
                image_url = None
                if img_elem:
                    image_url = img_elem.get('src') or img_elem.get('data-src')
                    if image_url and not image_url.startswith('http'):
                        image_url = urljoin(self.BASE_URL, image_url)
                
                # –û–ø–∏—Å–∞–Ω–∏–µ
                desc_elem = soup.find(['div', 'p'], class_=re.compile(r'description|content', re.I))
                description = desc_elem.get_text(strip=True) if desc_elem else ""
                
                # –õ–æ–∫–∞—Ü–∏—è
                location_elem = soup.find(['span', 'div'], class_=re.compile(r'location|city|town', re.I))
                location = location_elem.get_text(strip=True) if location_elem else ""
                
                # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–¥–∞–≤—Ü–µ
                seller_name = ""
                seller_url = ""
                seller_online_status = ""
                seller_listings_count = 0
                seller_registration_date = ""
                
                seller_elem = soup.find(['div', 'section'], class_=re.compile(r'seller|user|author', re.I))
                if seller_elem:
                    # –ò–º—è –ø—Ä–æ–¥–∞–≤—Ü–∞
                    name_elem = seller_elem.find(['span', 'a', 'p'], class_=re.compile(r'name|username', re.I))
                    if name_elem:
                        seller_name = name_elem.get_text(strip=True)
                        if name_elem.name == 'a' and name_elem.get('href'):
                            seller_url = urljoin(self.BASE_URL, name_elem['href'])
                    
                    # –°—Ç–∞—Ç—É—Å –æ–Ω–ª–∞–π–Ω
                    status_elem = seller_elem.find(['span', 'div'], class_=re.compile(r'online|status|active', re.I))
                    if status_elem:
                        seller_online_status = status_elem.get_text(strip=True)
                    
                    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
                    listings_elem = seller_elem.find(text=re.compile(r'annunci|pubblicati', re.I))
                    if listings_elem:
                        numbers = re.findall(r'\d+', listings_elem)
                        if numbers:
                            seller_listings_count = int(numbers[0])
                    
                    # –î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
                    reg_elem = seller_elem.find(text=re.compile(r'dal|da|registrato|pubblica da', re.I))
                    if reg_elem:
                        seller_registration_date = reg_elem.strip()
                
                # –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                date_elem = soup.find(['span', 'time'], class_=re.compile(r'date|published|time', re.I))
                published_date = date_elem.get_text(strip=True) if date_elem else ""
                
                # –ö–∞—Ç–µ–≥–æ—Ä–∏—è
                category_elem = soup.find(['span', 'a'], class_=re.compile(r'category|breadcrumb', re.I))
                category = category_elem.get_text(strip=True) if category_elem else ""
                
                return Listing(
                    listing_id=listing_id,
                    title=title,
                    price=price,
                    url=url,
                    image_url=image_url,
                    seller_name=seller_name,
                    seller_url=seller_url,
                    seller_online_status=seller_online_status,
                    location=location,
                    description=description,
                    published_date=published_date,
                    seller_listings_count=seller_listings_count,
                    seller_registration_date=seller_registration_date,
                    category=category
                )
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {e}")
            return None
    
    def _extract_price(self, price_text: str) -> float:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è —Ü–µ–Ω—ã"""
        try:
            # –£–¥–∞–ª—è–µ–º –≤—Å–µ –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä –∏ —Ç–æ—á–∫–∏/–∑–∞–ø—è—Ç–æ–π
            price_text = re.sub(r'[^\d,.]', '', price_text)
            # –ó–∞–º–µ–Ω—è–µ–º –∑–∞–ø—è—Ç—É—é –Ω–∞ —Ç–æ—á–∫—É
            price_text = price_text.replace(',', '.')
            return float(price_text) if price_text else 0.0
        except:
            return 0.0
    
    def _matches_filters(self, listing: Listing, settings: Dict[str, Any]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –æ–±—ä—è–≤–ª–µ–Ω–∏—è –±–∞–∑–æ–≤—ã–º —Ñ–∏–ª—å—Ç—Ä–∞–º"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–Ω—ã
        min_price = settings.get("min_price", 0)
        max_price = settings.get("max_price", float('inf'))
        
        if not (min_price <= listing.price <= max_price):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keywords = settings.get("keywords", [])
        if keywords:
            title_lower = listing.title.lower()
            if not any(keyword.lower() in title_lower for keyword in keywords):
                return False
        
        return True
    
    def _matches_seller_filters(self, listing: Listing, settings: Dict[str, Any]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ñ–∏–ª—å—Ç—Ä–∞–º –ø—Ä–æ–¥–∞–≤—Ü–∞"""
        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —É –ø—Ä–æ–¥–∞–≤—Ü–∞
        max_seller_listings = settings.get("max_seller_listings", float('inf'))
        if listing.seller_listings_count > max_seller_listings:
            return False
        
        # –ì–æ–¥ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–æ–¥–∞–≤—Ü–∞
        seller_year = settings.get("seller_registration_year")
        if seller_year and listing.seller_registration_date:
            year_match = re.search(r'\d{4}', listing.seller_registration_date)
            if year_match:
                reg_year = int(year_match.group())
                if reg_year < seller_year:
                    return False
        
        return True

